<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> PCA - Derivations and extensions | Charles Heller </title> <meta name="author" content="Charles Heller"> <meta name="description" content="In this project, I explore different methods for performing Principal Component Analysis with the goal of providing a deeper intuitive understanding of the method and its applicability to data science problems."> <meta name="keywords" content="neuroscience, data science, max planck institute"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://crheller.github.io/projects/2_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Charles</span> Heller </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">PCA - Derivations and extensions</h1> <p class="post-description">In this project, I explore different methods for performing Principal Component Analysis with the goal of providing a deeper intuitive understanding of the method and its applicability to data science problems.</p> </header> <article> <h2 id="background">Background</h2> <p>Thanks to the development of new experimental tools for measuring neural activity, neuroscience data is rapidly increasing in its <a href="https://www.nature.com/articles/nn.2731" rel="external nofollow noopener" target="_blank">dimensionality</a>. As a result, <a href="https://www.nature.com/articles/nn.3776" rel="external nofollow noopener" target="_blank">dimensionality reduction methods</a>, like PCA, are becoming more and more commonplace in neuroscience research. Therefore, when I started my PhD working with these types of neural datasets, I was looking for ways to deepen my understanding of PCA so that I could better understand how / when it could be applied to my data. In this project, I document some of the excercises that helped me to strengthen my grasp on the method.</p> <p>PCA is also widely used outside of neuroscience and its application include data compression, data visualization, latent variable discovery etc. Using open source tools, such as <code class="language-plaintext highlighter-rouge">sklearn</code>, today anyone can easily apply PCA to their data. Therefore, I hope that the following excercies might also be useful to others that are using PCA and are looking for ways to dig a little deeper into the method. A basic knowledge of PCA and linear algebra is assumed throughout the following sections.</p> <h4 id="tools-used">Tools used:</h4> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">numpy</span>
<span class="n">scipy</span>
<span class="n">sklearn</span>
<span class="n">matplotlib</span>
<span class="n">seaborn</span></code></pre></figure> <p>Full code available at: <a href="https://github.com/crheller/PCAdemos" rel="external nofollow noopener" target="_blank">https://github.com/crheller/PCAdemos</a></p> <h2 id="outline">Outline</h2> <ol> <li><a href="#basics">The basics</a></li> <li><a href="#edecomp">PCA as an eigendecomposition problem</a></li> <li><a href="#reconstruction">PCA as a reconstruction optimization problem</a></li> <li><a href="#sparse">Extensions of PCA - Sparse PCA</a></li> <li><a href="#summary">Summary</a></li> </ol> <h2 id="the-basics"> <a name="basics"></a>The basics</h2> <p>PCA is a linear dimensionality reduction method. The goal of dimensionalty reduction, in general, is to find a new represenation of the original data which maintains the data’s overall structure while also significantly reducing the number of dimensions needed to describe it. In the case of PCA, this is done by finding the ordered set of orthonormal basis vectors (called <strong>loadings</strong>) which capture the principal axes of variation in the original data. For every dataset of dimensionality \(D\), there exist \(D\) loading vectors which, together, fully capture the variance in the original data and can be thought of geometrically as describing a rotation of the original dataset. To reduce dimensionality, it is standard to discard a subset of loading vectors which capture only a small amount of variance. The number of dimensions to discard is determined by analysis of the <a href="https://en.wikipedia.org/wiki/Scree_plot" rel="external nofollow noopener" target="_blank">Scree plot</a> and can vary based on your given application. Finally, the data is <strong>transformed</strong> by projecting it onto the chosen set of loadings and using this new, low dimensional representation of the data, we can then <strong>reconstruct</strong> a “denoised”, low-rank version of the original data. The figure below gives a graphical intuition for these ideas, by applying PCA to a 2-D simulated dataset, \(\textbf{X}\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pca/pca_basic-480.webp 480w,/assets/img/pca/pca_basic-800.webp 800w,/assets/img/pca/pca_basic-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pca/pca_basic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="PCA basics" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example visualization of PCA applied to a 2-D data matrix, X. </div> <h2 id="pca-as-an-eigendecomposition-problem"> <a name="edecomp"></a>PCA as an eigendecomposition problem</h2> <p>The previous section gives a general geometrical intuition for what you can achieve with PCA. However, when I started this project I wanted to learn more about how the problem is actually solved. To do this, I tried to implement PCA myself using <code class="language-plaintext highlighter-rouge">numpy</code>.</p> <p>The most typical way PCA is solved is through eigendecomposition of the data covariance matrix, \(\Sigma\). This is the method used by most open source libraries, such as <code class="language-plaintext highlighter-rouge">sklearn</code>’s <code class="language-plaintext highlighter-rouge">decomposition.PCA</code>. Here, I step through this approach using “low level” linear algebra operations available in <code class="language-plaintext highlighter-rouge">numpy</code> and show that it is able to exactly reproduce the output of <code class="language-plaintext highlighter-rouge">sklearn</code>.</p> <h4 id="data-generation">Data generation</h4> <p>To illustrate this approach, I again used a randomly generated 2-D synthetic data, \(X\) as above. To simplify things slightly, I generated mean-centered data (input data to PCA should always be mean-centered).</p> <h4 id="step-1---compute-the-covariance-matrix">Step 1 - Compute the covariance matrix</h4> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cov</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># use numpy covariance function to compute covariance matrix of data</span></code></pre></figure> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pca/data_and_cov_matrix-480.webp 480w,/assets/img/pca/data_and_cov_matrix-800.webp 800w,/assets/img/pca/data_and_cov_matrix-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pca/data_and_cov_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="pca data" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Synthetic input data. Right: Covariance matrix of synthetic data. </div> <h4 id="step-2---perform-eigendecomposition">Step 2 - Perform eigendecomposition</h4> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eig</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span> 

<span class="c1"># sort according to variance explained
</span><span class="n">sort_args</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">sort_args</span><span class="p">]</span>
<span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">sort_args</span><span class="p">]</span></code></pre></figure> <p>The eigenvectors from this decomposition correspond to PC loadings and the eigenvalues are directly related to the amount of data variance along each of the corresponding eigenvectors. Thus, we can use them to calculate the % variance explained by each principal component, as shown in step 3.</p> <h4 id="step-3---compute-fraction-variance-explained-by-each-component-using-eigenvalues">Step 3 - Compute fraction variance explained by each component using eigenvalues</h4> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">var_explained</span> <span class="o">=</span> <span class="n">eigenvalues</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span></code></pre></figure> <h4 id="step-4---verify-that-we-have-reproduced-sklearn-results">Step 4 - Verify that we have reproduced <code class="language-plaintext highlighter-rouge">sklearn</code> results</h4> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># sklearn pca
</span><span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">)</span></code></pre></figure> <p>Below, I show that the eigenvectors are identical to the components (loadings) returned by <code class="language-plaintext highlighter-rouge">sklearn</code>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># cosine similarity -- value of 1 means the two vectors are identical. 
# Thus the inner product should return the identity matrix if the solutions match and all components are orthogonal
</span><span class="n">dp</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">.(</span><span class="n">eigenvectors</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">)</span> 
<span class="nf">print</span><span class="p">(</span><span class="n">dp</span><span class="p">)</span>

<span class="nf">array</span><span class="p">([[</span> <span class="mf">1.00000000e+00</span><span class="p">,</span>  <span class="mf">1.97596122e-16</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">2.21149847e-16</span><span class="p">,</span>  <span class="mf">1.00000000e+00</span><span class="p">]])</span></code></pre></figure> <p>I also verified that the eigenvalue method for measuring % variance explained matches with <code class="language-plaintext highlighter-rouge">sklearn</code>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Eigendecomposition, variance explained ratio: </span><span class="si">{</span><span class="n">var_explained</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">sklearn, variance explained ratio: </span><span class="si">{</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">Eigendecomposition</span><span class="p">,</span> <span class="n">variance</span> <span class="n">explained</span> <span class="n">ratio</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.88977678</span> <span class="mf">0.11022322</span><span class="p">]</span>
<span class="n">sklearn</span><span class="p">,</span> <span class="n">variance</span> <span class="n">explained</span> <span class="n">ratio</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.88977678</span> <span class="mf">0.11022322</span><span class="p">]</span></code></pre></figure> <p>Finally, I used the first PC to reconstruct the original data using both methods and confirmed that, as expected, we get identical results:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">e_reconstructed</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">@</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]].</span><span class="n">T</span>
<span class="n">sk_reconstructed</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">:].</span><span class="n">T</span> <span class="o">@</span> <span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">:]</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">sum of reconstruction differences: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">e_reconstructed</span> <span class="o">-</span> <span class="n">sk_reconstructed</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nb">sum</span> <span class="n">of</span> <span class="n">reconstruction</span> <span class="n">differences</span><span class="p">:</span> <span class="o">-</span><span class="mf">1.8512968935624485e-14</span></code></pre></figure> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pca/eig_vs_sklearn_summary-480.webp 480w,/assets/img/pca/eig_vs_sklearn_summary-800.webp 800w,/assets/img/pca/eig_vs_sklearn_summary-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pca/eig_vs_sklearn_summary.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="loading similarity" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Summary results of eigendecomposition and sklearn approaches to PCA. Both yield identical results. </div> <h2 id="pca-as-a-reconstruction-optimization-problem"> <a name="reconstruction"></a>PCA as a reconstruction optimization problem</h2> <p>In the previous section, I showed that PCA can be formulated as an eigendecomposition problem. This approach highlighted that the covariance matrix \(\Sigma\), is a critical part of PCA. An alternative approach, however, is to formulate PCA as an optimization problem where the goal is to find the loading vector(s) that minimize the low-rank reconstruction error of the data. From an efficiency standpoint, this does not make much sense given that I just demonstrated the eigendeomposition formulation can be solved extremely efficiently. However, demonstrating that PCA can be posed as an optimization problem offers a couple of advantages. One, from an educational standpoint, it helped me to drive home the point that the objective of PCA is to find the loading vectors that can most accurately reconstruct the original data. Two, it makes it clear that we actually have flexibility to modify the objective function in order to suit our particular analysis needs. I will explore this flexibility in the following section. In this section, I demonstrate my implementation of standard PCA using <code class="language-plaintext highlighter-rouge">scipy</code>.</p> <p>To perform PCA, we seek to minimize the objective function <code class="language-plaintext highlighter-rouge">frob</code> - the squared <a href="https://mathworld.wolfram.com/FrobeniusNorm.html" rel="external nofollow noopener" target="_blank">Frobenius norm</a> of the difference between the reconstructed data and the original data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define objective function
</span><span class="k">def</span> <span class="nf">frob</span><span class="p">(</span><span class="n">pc</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="c1"># reconstruct rank-1 view of X
</span>    <span class="n">recon</span> <span class="o">=</span> <span class="nf">reconstruct</span><span class="p">(</span><span class="n">pc</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="c1"># compute error (sq. frob. norm of err in reconstruction)
</span>    <span class="n">err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">X</span><span class="o">-</span><span class="n">recon</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="sh">"</span><span class="s">fro</span><span class="sh">"</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="c1"># sq. forbenius norm
</span>    <span class="c1"># normalize by sq. frob norm of data, X
</span>    <span class="n">err</span> <span class="o">=</span> <span class="n">err</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">err</span>
</code></pre></div></div> <p>Subject to the constraint that each loading vector has a magnitude of 1:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">constraint</span><span class="p">(</span><span class="n">pc</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">pc</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
</code></pre></div></div> <p>Before fitting this model, I also defined a callback function in order to monitor the progress of my fitting procedure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define a callback function to monitor fitting progress
</span><span class="k">def</span> <span class="nf">callbackF</span><span class="p">(</span><span class="n">Xi</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">Nfeval</span>
    <span class="k">global</span> <span class="n">loss</span>
    <span class="k">global</span> <span class="n">parms</span>
    <span class="n">parms</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">Xi</span><span class="p">)</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="nf">frob</span><span class="p">(</span><span class="n">Xi</span><span class="p">,</span> <span class="n">Xfit</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Nfeval: </span><span class="si">{</span><span class="n">Nfeval</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="n">ll</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">Nfeval</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>Above, I have explicitly defined a constraint that ensures each loading vector has a magnitude of one. The second constraint that we must also enforce is that all loading vectors are orthogonal. To do this, I decided to perform the fitting in an iterative fashion. That is, I loop over principal components and fit one loading vector at a time. By construction, the first loading vector I find will explain the maximal amount of variance in the data. Thus, if I use this fitted loading on each iteration to <a href="">deflate</a> the target matrix \(\textbf{X}\) by subtracting the rank-1 reconstruction, I remove all variance associated with it. This means I am guaranteed that the next loading vector I find will be orthogonal to all those that were fit before it. The procedure for this is shown below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># fit model -- iterate over components, fit, deflate, fit next PC
</span>
<span class="n">constraints</span> <span class="o">=</span> <span class="p">({</span>
    <span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">eq</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">fun</span><span class="sh">'</span><span class="p">:</span> <span class="n">constraint</span>
<span class="p">})</span>
<span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">components_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n_components</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">Xfit</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span> 
<span class="n">loss_optim</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">params_optim</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_components</span><span class="p">):</span>
    <span class="n">Nfeval</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">parms</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># initialize the PC
</span>    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">x0</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    
    <span class="c1"># find optimal PC using scpiy's minimize
</span>    <span class="n">result</span> <span class="o">=</span> <span class="nf">minimize</span><span class="p">(</span><span class="n">frob</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">Xfit</span><span class="p">,),</span> <span class="n">callback</span><span class="o">=</span><span class="n">callbackF</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">SLSQP</span><span class="sh">'</span><span class="p">,</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">)</span>
    
    <span class="c1"># deflate X
</span>    <span class="n">Xfit</span> <span class="o">=</span> <span class="n">Xfit</span> <span class="o">-</span> <span class="nf">reconstruct</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">Xfit</span><span class="p">)</span>

    <span class="c1"># save resulting PC
</span>    <span class="n">components_</span><span class="p">[</span><span class="n">component</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">x</span>
    
    <span class="c1"># save all intermediate PCs and loss during the optimization procedure
</span>    <span class="n">loss_optim</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">params_optim</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">parms</span><span class="p">)</span>
</code></pre></div></div> <p>From the animation below, it is clear that the estimate of the first loading vector converges relatively quickly to the true solution given by <code class="language-plaintext highlighter-rouge">sklearn</code> using this procedure.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pca/optim-480.webp 480w,/assets/img/pca/optim-800.webp 800w,/assets/img/pca/optim-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pca/optim.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="pca optimization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Animation of PCA fitting procedure for the first loading vector. Left: input data shown in gray, estimate of the first loading vector shown in black, true first loading vector shown in red. Right: Reconstruction error as a function of optimization steps (N feval - Number of function evaluations). </div> <p>In addition to testing the above approach with simple 2-D data, I explored higher dimensional data, as well. Even with this high(er)-D data, the optimization approach is typically quite stable. However, especially for the lower variance principal components, this approach does not always guarantee finding the true solution. Thus, while illustrative, there is really no reason to use this approach for standard PCA over methods like the eigendecomposition approach.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pca/highD_optim-480.webp 480w,/assets/img/pca/highD_optim-800.webp 800w,/assets/img/pca/highD_optim-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pca/highD_optim.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="pca high-d optimization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Scree plot of simulated 15-D data. Right: Cosine similarity between loadings calculated using `sklearn` vs. optimized loadings fit using the procedure described above. If the solutions were identical, we would expect 1's along the diagonal and zeros everywhere else. </div> <h2 id="extensions-of-pca---sparse-pca"> <a name="sparse"></a>Extensions of PCA - Sparse PCA</h2> <p>One advantage of thinking of PCA as an optimization problem is that one realizes how closely it parallels linear regression. This is useful because there are a variety of common extensions that exist for regression models. For example, introducing a sparsity constraint on the fitted regression coefficients (which are analogous to loading vectors, in the case of PCA). Identifying sparse loadings weights can be useful for interpretability. I was particularly interested in exploring this application because in neuroscience data the goal of population level neural data analysis is often to identify a specific subpopulation(s) of neurons that are involved in the process you are studying (e.g., encoding a sensory variable, driving behavior etc.). In my experience, this is tough to achieve with standard PCA because each loading vector is typically composed of combinations of all input neurons. Thus, sparse PC loadings have the potential to be much more biologically interpretable.</p> <p>Before jumping into my implementation, I should also note that a variety of solutions have been proposed for performing Sparse PCA and packages such as <code class="language-plaintext highlighter-rouge">sklearn</code> often offer a version of <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html" rel="external nofollow noopener" target="_blank">Sparse PCA</a>.</p> <p>To implement sparse PCA “from scratch,” it is helpful to first briefly dig just a little bit into the math of the PCA objective function. In standard PCA, the goal is to minimize reconstruction error. Mathematically this can be formulated as:</p> \[\|\textbf{X} - \textbf{X}WW^T\|_{F}^{2}\] <p>Where \(\textbf{X}\) represents our original data, \(W\) represents a single loading vector, and \(\|\cdot\|_{F}^{2}\) represents the squared Frobenius norm. Thus, minimizing this function finds \(W\) such that \(\textbf{X}WW^T\) is the best possible reconstruction of \(\textbf{X}\), subject to the constraint that all \(W\) form an <a href="https://en.wikipedia.org/wiki/Orthonormality" rel="external nofollow noopener" target="_blank">orthonomal basis set</a>. This means that all vectors have a magnitude of one and are orthogonal. Mathematically, this can be expressed as \(WW^T=I\).</p> <p>From this starting point, it is straightforward to introduce a sparsity penalty. One way to do this is using the <a href="https://mathworld.wolfram.com/L1-Norm.html" rel="external nofollow noopener" target="_blank">L1 norm</a>. Using the L1 norm to penalize non-sparse loading vectors, our new objective function becomes:</p> \[\|\textbf{X} - \textbf{X}WW^T||_F^2 + \lambda\sum_{i=1}^{n}\|\textbf{w}_i\|_1\] <p>Where the second term, \(\sum_{i=1}^{n}\|\textbf{w}_i\|_1\) is the L1 norm and \(\lambda\) is a tunable hyperparameter that controls the level of sparsity. This new objective function is now very similar to <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)" rel="external nofollow noopener" target="_blank">LASSO</a> regression. In more general terms, this adds a second “goal” that the fitter is trying to achieve – ensure that the loading weights are sparse.</p> <p>To get a feel for how this might work in practice, I modified my <a href="#reconstruction">previous implementation</a> of the objective function to include the L1 norm as shown below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># wrapper around frob error function that adds a L1 regularization penalty
</span><span class="k">def</span> <span class="nf">sparse_wrapper</span><span class="p">(</span><span class="n">pc</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">lam</span><span class="p">):</span>
    <span class="c1"># compute reconstruction error
</span>    <span class="n">err</span> <span class="o">=</span> <span class="nf">frob</span><span class="p">(</span><span class="n">pc</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="c1"># add L1 regularization
</span>    <span class="n">err</span> <span class="o">=</span> <span class="n">err</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">pc</span><span class="p">))</span> <span class="o">*</span> <span class="n">lam</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">err</span>
</code></pre></div></div> <p>I then simulated datasets and fit my Sparse PCA model for a range of sparsity constraints. The effect of sparsity on the fitted loading vectors, and on the variance explained by the reconstruction, is shown in the figure below for one such simulation:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pca/sparse-480.webp 480w,/assets/img/pca/sparse-800.webp 800w,/assets/img/pca/sparse-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pca/sparse.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="pca high-d optimization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Scree plot of simulated 15-D data for standard PCA (blue) and for Sparse PCA with a range of sparsity constraints. Right, top: First principal component loading vectors for each value of sparsity. Right, bottom: Same, for second principal component. Dark red = 1, dark blue = -1, white = 0. From this, it is clear that increasing lambda leads to more 0 weight entries in the loading vectors. </div> <p>As expected, increasing \(\lambda\) leads to finding loading vectors with a more sparse set of weights. However, it also quickly reduces the amount of variance explained in the data by each of the top principal components. This is why Sparse PCA, while sometimes useful for finding interpretable loadings, is usually not a great method for other applications of PCA, such as data compression.</p> <h2 id="summary"> <a name="summary"></a>Summary</h2> <p>In this project, I briefly highlighted the basics of PCA. In addition, I demonstrated two separate methods for solving the problem from “first principles” and touched on the some of the advantages and disadvantages of each method. Finally, I showed that implementing a version of Sparse PCA is relatively straightforward, when PCA is thought of as a reconstruction optimization problem. For me, this was one of the biggest take homes - that other dimensionality reduction methods, such as Sparse PCA, Non-negative matrix factorization, Factor Analysis, etc., and their relationship to PCA, can be more easily understood when considering the underlying objective function of PCA.</p> <p>All the code used in this project is saved and available on my <a href="https://github.com/crheller/PCAdemos" rel="external nofollow noopener" target="_blank">github</a>.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Charles Heller. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>