<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> PCA - Derivations, extensions, and limitations | Charles R. Heller </title> <meta name="author" content="Charles R. Heller"> <meta name="description" content="In this project, I explore different methods for performing Principal Component Analysis with the goal of providing a deeper intuitive understanding of the method and its applicability to data science problems."> <meta name="keywords" content="neuroscience, data science, max planck institute"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://crheller.github.io/projects/2_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Charles</span> R. Heller </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">PCA - Derivations, extensions, and limitations</h1> <p class="post-description">In this project, I explore different methods for performing Principal Component Analysis with the goal of providing a deeper intuitive understanding of the method and its applicability to data science problems.</p> </header> <article> <h2 id="background">Background</h2> <p>Principal component analysis, typically referred to as PCA, is a popular dimensionality reduction technique utlized across many fields. Applications of PCA include data compression, data visualization, and latent variable discovery. Using open source tools, such as <code class="language-plaintext highlighter-rouge">sklearn</code>, today anyone can easily apply PCA to their data. However, this ease of access means that PCA is often applied without full consideration of whether or not it is the appropriate method for a given data analysis problem.</p> <p>Here, we dig a bit deeper by exploring some of the different possible methods for implementing PCA from first principles. Our goal is not to go through the mathematical derivations in detail, but rather to give a general and geometric intuition for PCA, how and when it can be applied to data, and ways to extend / customize the method depending on your data analysis needs.</p> <h4 id="tools-used">Tools used:</h4> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">numpy</span>
<span class="n">scipy</span>
<span class="n">sklearn</span>
<span class="n">matplotlib</span>
<span class="n">seaborn</span></code></pre></figure> <p>Full code available at: (I will add link)</p> <h2 id="outline">Outline</h2> <ol> <li><a href="#basics">The basics</a></li> <li><a href="#edecomp">PCA as an eigendecomposition problem</a></li> <li><a href="#reconstruction">PCA as a reconstruction optimization problem</a></li> <li><a href="#sparse">Extensions of PCA - Sparse PCA</a></li> <li><a href="#limitations">Limitations of PCA for latent variable disovery</a></li> <li><a href="#summary">Summary</a></li> </ol> <h2 id="the-basics"> <a name="basics"></a>The basics</h2> <p>PCA is a linear dimensionality reduction method. The goal of dimensionalty reduction, in general, is to find a represenation of the original data which maintains the data’s overall structure while reducing the number of dimensions needed to describe it. In the case of PCA, this is done by finding the ordered set of orthonormal <strong>basis</strong> vectors (called <strong>loadings</strong>) which capture the principal axes of variation in the original data. To reduce the dimensionality, the loading vectors which capture the least amount of variance are discarded. The data is then <strong>transformed</strong> by projecting it onto the remaining loadings. Using this new, low dimensional representation of the data, it is then possible to <strong>reconstruct</strong> a “denoised”, low-rank version of the original data. This basic idea is illustrated graphically with the simulated data below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pca/pca_basic-480.webp 480w,/assets/img/pca/pca_basic-800.webp 800w,/assets/img/pca/pca_basic-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pca/pca_basic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="PCA basics" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example visualization of PCA applied to a 2-D data matrix, X. </div> <h2 id="pca-as-an-eigendecomposition-problem"> <a name="edecomp"></a>PCA as an eigendecomposition problem</h2> <p>The most typical way PCA is solved is by performing an eigendecomposition of the raw data covariance matrix. This is the method used by most open source libraries, such as <code class="language-plaintext highlighter-rouge">sklearn.decomposition.PCA</code>. Here, we step through this approach using “low level” linear algebra operations available in <code class="language-plaintext highlighter-rouge">numpy</code> to reproduce the output of <code class="language-plaintext highlighter-rouge">sklearn</code>.</p> <h4 id="data-generation">Data generation</h4> <p>To illustrate this approach, we will use randomly generated 2-D synthetic data as above. To simplify our approach slightly, we generate mean-centered data.</p> <h4 id="step-1---compute-the-covariance-matrix">Step 1 - Compute the covariance matrix</h4> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cov</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># use numpy covariance function to compute covariance matrix of data</span></code></pre></figure> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pca/data_and_cov_matrix-480.webp 480w,/assets/img/pca/data_and_cov_matrix-800.webp 800w,/assets/img/pca/data_and_cov_matrix-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pca/data_and_cov_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="pca data" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Synthetic input data. Right: Covariance matrix of synthetic data. </div> <h4 id="step-2---perform-eigendecomposition">Step 2 - Perform eigendecomposition</h4> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span> 

<span class="c1"># sort according to variance explained
</span><span class="n">sort_args</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="n">sort_args</span><span class="p">]</span>
<span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">sort_args</span><span class="p">]</span></code></pre></figure> <h4 id="step-3---compute-fraction-variance-explained-by-each-component">Step 3 - Compute fraction variance explained by each component</h4> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">var_explained</span> <span class="o">=</span> <span class="n">eigenvalues</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span></code></pre></figure> <h4 id="step-4---verify-that-we-have-reproduced-sklearn-results">Step 4 - Verify that we have reproduced <code class="language-plaintext highlighter-rouge">sklearn</code> results</h4> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># sklearn pca
</span><span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">)</span></code></pre></figure> <p>The eigenvectors we found are identical to the components returned by <code class="language-plaintext highlighter-rouge">sklearn</code>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># the dot product of our eigenvectors and `sklearn`'s components should return the identity matrix
</span><span class="n">dp</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">.(</span><span class="n">eigenvectors</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dp</span><span class="p">)</span>

<span class="nf">array</span><span class="p">([[</span> <span class="mf">1.00000000e+00</span><span class="p">,</span>  <span class="mf">1.97596122e-16</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">2.21149847e-16</span><span class="p">,</span>  <span class="mf">1.00000000e+00</span><span class="p">]])</span></code></pre></figure> <p>We have correctly measured the variance explained by each component.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Eigendecomposition, variance explained ratio: </span><span class="si">{</span><span class="n">var_explained</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">sklearn, variance explained ratio: </span><span class="si">{</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">Eigendecomposition</span><span class="p">,</span> <span class="n">variance</span> <span class="n">explained</span> <span class="n">ratio</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.88977678</span> <span class="mf">0.11022322</span><span class="p">]</span>
<span class="n">sklearn</span><span class="p">,</span> <span class="n">variance</span> <span class="n">explained</span> <span class="n">ratio</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.88977678</span> <span class="mf">0.11022322</span><span class="p">]</span></code></pre></figure> <p>Using the first PC to reconstruct our data, we get identical results.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">e_reconstructed</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">@</span> <span class="n">eigenvectors</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]].</span><span class="n">T</span>
<span class="n">sk_reconstructed</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">:].</span><span class="n">T</span> <span class="o">@</span> <span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">:]</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">sum of reconstruction differences: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">e_reconstructed</span> <span class="o">-</span> <span class="n">sk_reconstructed</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nb">sum</span> <span class="n">of</span> <span class="n">reconstruction</span> <span class="n">differences</span><span class="p">:</span> <span class="o">-</span><span class="mf">1.8512968935624485e-14</span></code></pre></figure> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pca/eig_vs_sklearn_summary-480.webp 480w,/assets/img/pca/eig_vs_sklearn_summary-800.webp 800w,/assets/img/pca/eig_vs_sklearn_summary-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pca/eig_vs_sklearn_summary.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="loading similarity" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Summary results of eigendecomposition and sklearn approaches to PCA. Both yield identical results. </div> <h2 id="pca-as-a-reconstruction-optimization-problem"> <a name="reconstruction"></a>PCA as a reconstruction optimization problem</h2> <p>Instead of using eigendecomposition, it is also possible to formulate PCA as an optimization problem in which the objective is to find the set of basis vectors that minimize the low-rank reconstruction error of the data. From an efficiency standpoint, this does not make a lot of sense given that we have already demonstrated that there exists a closed form solution to PCA that can be solved extremely efficiently. However, demonstrating that PCA can be posed as an optimization problem helps to drive home the point that the fundamental objective of PCA is to find the basis vectors that capture as much variance as possible in the original data. Furthermore, it allows us the flexibility to modify the objective function of the optimization in order to suit our particular analysis needs. But more on that in the following section.</p> <p>To perform PCA, we seek to minimize the objective function <code class="language-plaintext highlighter-rouge">frob</code> - the squared <a href="https://mathworld.wolfram.com/FrobeniusNorm.html" rel="external nofollow noopener" target="_blank">Frobenius norm</a> of the difference between the reconstructed data and the original data:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># define objective function
</span><span class="k">def</span> <span class="nf">frob</span><span class="p">(</span><span class="n">pc</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="c1"># reconstruct rank-1 view of X
</span>    <span class="n">recon</span> <span class="o">=</span> <span class="nf">reconstruct</span><span class="p">(</span><span class="n">pc</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="c1"># compute error (sq. frob. norm of err in reconstruction)
</span>    <span class="n">err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">X</span><span class="o">-</span><span class="n">recon</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="sh">"</span><span class="s">fro</span><span class="sh">"</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="c1"># sq. forbenius norm
</span>    <span class="c1"># normalize by sq. frob norm of data, X
</span>    <span class="n">err</span> <span class="o">=</span> <span class="n">err</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">err</span></code></pre></figure> <p>Next, we also define a callback function to monitor the progress of our fitting procedure.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># define a callback function to monitor fitting progress
</span><span class="k">def</span> <span class="nf">callbackF</span><span class="p">(</span><span class="n">Xi</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">Nfeval</span>
    <span class="k">global</span> <span class="n">loss</span>
    <span class="k">global</span> <span class="n">parms</span>
    <span class="n">parms</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">Xi</span><span class="p">)</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="nf">frob</span><span class="p">(</span><span class="n">Xi</span><span class="p">,</span> <span class="n">Xfit</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Nfeval: </span><span class="si">{</span><span class="n">Nfeval</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="n">ll</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">Nfeval</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure> <p>Finally, we perform the optimization. In order to ensure that our fitted basis vectors are orthogonal, as required by PCA, we perform the fitting in an iterative fashion. That is, we loop over principal components in order of decreasing variance explained and on each iteration we deflate the target matrix <strong>X</strong> by the rank-1 reconstruction for each principal component. This ensures that on the next iteration we find a basis vector that is orthogonal to all those that were fit before it. The procedure for this is shown below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># fit model -- iterate over components, fit, deflate, fit next PC
</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">components_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n_components</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">Xfit</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span> <span class="c1"># we iteratively deflate X while fitting to ensure fitted PCs are orthogonal
# save loss / parameters during fit
</span><span class="n">loss_optim</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">params_optim</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_components</span><span class="p">):</span>
    <span class="n">Nfeval</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">parms</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># initialize the PC
</span>    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">x0</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    
    <span class="c1"># find optimal PC using scpiy's minimize
</span>    <span class="n">result</span> <span class="o">=</span> <span class="nf">minimize</span><span class="p">(</span><span class="n">nmse</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">Xfit</span><span class="p">,),</span> <span class="n">callback</span><span class="o">=</span><span class="n">callbackF</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">Nelder-Mead</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># deflate X
</span>    <span class="n">Xfit</span> <span class="o">=</span> <span class="n">Xfit</span> <span class="o">-</span> <span class="nf">reconstruct</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">Xfit</span><span class="p">)</span>

    <span class="c1"># save resulting PC
</span>    <span class="n">components_</span><span class="p">[</span><span class="n">component</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">x</span>
    
    <span class="c1"># save all intermediate PCs and loss during the optimization procedure
</span>    <span class="n">loss_optim</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">params_optim</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">parms</span><span class="p">)</span>
</code></pre></div></div> <p>In the animation below, we visualize the fitting process. We can see that we converge relatively quickly to the true solution for the first loading vector.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pca/optim-480.webp 480w,/assets/img/pca/optim-800.webp 800w,/assets/img/pca/optim-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pca/optim.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="pca optimization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Animation of PCA fitting procedure for the first loading vector. Left: input data shown in gray, estimate of the first loading vector shown in black, true first loading vector shown in red. Right: Reconstruction error as a function of optimization steps (N feval - Number of function evaluations). </div> <h2 id="extensions-of-pca---sparse-pca"> <a name="sparse"></a>Extensions of PCA - Sparse PCA</h2> <p>One useful advantage of advantage of thinking of PCA as an optimization problem is that it draws a close parallel to linear regression, which has been extended in a variety of ways to suit different analysis problems - such as introducing sparsity to the fitted regression coefficients (analogous to loading vectors in the case of PCA). Identifying sparse loadings (many weights are equal to 0) can be useful for interpretability. For example, if you have a very high dimensional dataset you might want to identify only a small subset of the input variables which contribute to variance, rather than a combination of all the variables which can be difficult to interpret. In order to understand how we can implement this, it is helpful to briefly highlight a bit of the underlying math.</p> <p>In standard PCA, we seek to minimize the objective function described in code the previous section. This objective function can be written mathematically as:</p> <p>Where \(\textbf{X}\) represents our original data, \(W\) represents a principal component (i.e., a basis or “loading” vector), and \(||\cdot||_F^2\) represents the squared Frobenius norm. Thus, the goal is to find \(W\) such that we minimize the difference between \(\textbf{X}\) and its rank-1 reconstruction: \(\textbf{X}WW^T\).</p> <p>In order to arrive at a form of Sparse PCA, all we need to do is tack on a sparsity penalty to our objective function. One way to do this is using the <a href="https://mathworld.wolfram.com/L1-Norm.html" rel="external nofollow noopener" target="_blank">L1 norm</a>. Our new objective function then becomes: –&gt;</p> <p>To see this in action, let’s take a look at a quick example (synethic) dataset:</p> <h2 id="limitations-of-pca-for-latent-variable-disovery"> <a name="limitations"></a>Limitations of PCA for latent variable disovery</h2> <p>Fake height / weight data. Goal is to discover the latent relationship between height / weight. But, imagine we don’t know male vs. female. This latent factor could affect the relationship between height / weight that PCA discovers. Thus, if the latent dimensions of variation are not orthogonal, PCA fails to accurately capture the underlying latent relationships.</p> <p>Non linear latent relationships are not well captured by PCA.</p> <h2 id="summary"> <a name="summary"></a>Summary</h2> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Charles R. Heller. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>