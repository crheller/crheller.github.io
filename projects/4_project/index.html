<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-XDX3RFLYFH"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-XDX3RFLYFH");</script> <meta name="google-site-verification" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Flexible sensory processing in whole-brain circuits | Charles Heller </title> <meta name="author" content="Charles Heller"> <meta name="description" content="This is an ongoing research project investigating how the brains of larval zebrafish flexibly process incoming sensory information. I use cross-decomposition methods (Reduced Rank Regression) to investigate how populations of neurons share information to drive behavior."> <meta name="keywords" content="neuroscience, data science, max planck institute, charlie"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://crheller.github.io/projects/4_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Charles</span> Heller </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Flexible sensory processing in whole-brain circuits</h1> <p class="post-description">This is an ongoing research project investigating how the brains of larval zebrafish flexibly process incoming sensory information. I use cross-decomposition methods (Reduced Rank Regression) to investigate how populations of neurons share information to drive behavior.</p> </header> <article> <h2 id="introduction">Introduction</h2> <p>One of the main goals of my postdoc is to study how neural circuits flexibly process sensory information. For example, when we are asleep, how does our brain completely “tune out” sounds that we react and respond to when we are awake? Presumably our ears still work and are providing information about the sound to our brain, yet, somehow, the brain determines that certain sounds are not behaviorally relevant during a sleep state and therefore we do not need to take any action, or, in fact, even consciously perceive them. Thus, the goal of this research is to identify the neural circuit computations that underlie this type of flexible processing ability. In the future, this could help inform the design of better, adaptive sensory prosthetics that dynamically filter incoming sensory information based on the cognitive and behavioral demands of users.</p> <p>In this post, I provide a bird’s eye view of the approach I have been taking to answer these questions and highlight some of my preliminary findings. Before getting into the data analysis and results, I first provide a general description of the experimental approach and the data. My goal is that with this background information, the results will be interpretable to a general scientific and/or computational audience.</p> <p>In order to keep this post from getting too long, some details of the analysis and results are omitted. For those comfortable with these topics and interested in digging a bit deeper, please feel free to check out my <a href="../../assets/pdf/fens_poster.pdf">poster</a> on this topic which I recently presented at <a href="https://fensforum.org/" rel="external nofollow noopener" target="_blank">FENS 2024</a> in Vienna.</p> <h2 id="outline">Outline</h2> <ol> <li><a href="#background">Background</a></li> <li><a href="#methods">Experimental approach</a></li> <li><a href="#data">The data</a></li> <li><a href="#results">Results</a></li> <li><a href="#future">Conclusions and future directions</a></li> </ol> <h2 id="background"> <a name="background"></a>Background</h2> <p>During my PhD, I studied how changes in an animal’s internal state (e.g., asleep vs. awake) modulate the way neurons in the brain respond to auditory stimuli. You can read more about an example of this work <a href="https://elifesciences.org/reviewed-preprints/89936" rel="external nofollow noopener" target="_blank">here</a>. Due to technical limitations of working with “larger” animals, in these experiments I was restricted to measuring the activity of only a handful (10s to 100s) of neurons in a single brain region. One drawback of this approach is that it prevents being able to contextualize the observed patterns of activity within the larger brain network. The brain is a hugely complex interconnected system consisting of multiple brain regions. To fully understand the function of any one region it is important not only measure activity in that region, but also in all the other regions that it communicates with.</p> <p>Thus, for my postdoc I chose to study the brain of a relatively small animal, the larval zebrafish. Zebrafish have comparatively small brains, yet, possess many molecularly homologous cell types and brain regions to mammals. In addition, they have a well-studied, well-defined behavioral repertoire. Importantly, methods exists to record their whole-brain activity at high spatial resolution during behavior. This type of comprehensive data presents the opportunity to build more sophisticated models that map neural activity across the entire brain to behavior.</p> <h2 id="experimental-approach"> <a name="methods"></a>Experimental approach</h2> <p>To measure neural activity across the brain, we use an approach called <a href="https://en.wikipedia.org/wiki/Calcium_imaging" rel="external nofollow noopener" target="_blank">calcium imaging</a>. Briefly, we use genetically modified animals with calcium inidicators encoded into all of their neurons. These indicators report the presence of intracellular calcium concentrations (a readout of neural activity) by changing their fluorescence intensity. We can measure changes in fluorescence over time using a <a href="https://en.wikipedia.org/wiki/Fluorescence_microscope" rel="external nofollow noopener" target="_blank">fluorescent microscope</a>.</p> <p>In our lab, we are interested in the intersection between neural activity and behavior. Most micropscopy techniques, however, require that the specimen being imaged is immobile. This makes the joint study of neural activity and natural behavior difficult, or even impossible. To circumvent this, our lab recently built a state-of-the-art <a href="https://www.nature.com/articles/nmeth.4429" rel="external nofollow noopener" target="_blank">microscope</a> that permits whole-brain imaging, at single cell resolution, in freely behaving fish.</p> <div class="row justify-content-sm-center"> <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/roliFlexible/diff_pic-480.webp 480w,/assets/img/roliFlexible/diff_pic-800.webp 800w,/assets/img/roliFlexible/diff_pic-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/roliFlexible/diff_pic.png" class="img-fluid z-depth-1" width="100%" height="auto" title="microscope" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/roliFlexible/diff_vid-480.webp 480w,/assets/img/roliFlexible/diff_vid-800.webp 800w,/assets/img/roliFlexible/diff_vid-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/roliFlexible/diff_vid.gif" class="img-fluid z-depth-1" width="100%" height="auto" title="diff_gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Schematic of closed-loop tracking microscope enabling whole-brain imaging in a freely swimming zebrafish larva. Right: Video showing simultaneous fish behavior (left) and neural activity across the brain measured with calcium imaging (right). </div> <p>In this project, I used the setup shown above to study how the brain processes a simple visual stimulus during sleep vs. wake conditions. The visual stimulus I chose is referred to as a “dark flash” and corresponds to a 100-percent contrast “off” stimulus - That is, I simply turn off the white lights illuminating the fish’s behavioral arena so that the fish is briefly in the dark. In awake, alert fish, this stimulus is known to reliably ellicit high amplitude turning behavior, as shown in the time-lapsed image below.</p> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/roliFlexible/darkflash_resp-480.webp 480w,/assets/img/roliFlexible/darkflash_resp-800.webp 800w,/assets/img/roliFlexible/darkflash_resp-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/roliFlexible/darkflash_resp.png" class="img-fluid z-depth-0" width="100%" height="auto" title="resp" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Time-lapsed image of larval zebrafish responding to a dark flash stimulus. Left panel, 0.4 s prior to lights turning off. Right panel, 0.4 s following lights turning off. The stimulus was repeated k times during a single experiment. </div> <p>Interestingly, as part of a <a href="https://www.biorxiv.org/content/10.1101/2023.08.28.555077v1.full" rel="external nofollow noopener" target="_blank">prior study</a> which I co-led, we discovered that the same dark flash stimulus rarely ellicits a behavioral response when animals are in a quiescent, sleep-like state.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/roliFlexible/state_resp-480.webp 480w,/assets/img/roliFlexible/state_resp-800.webp 800w,/assets/img/roliFlexible/state_resp-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/roliFlexible/state_resp.png" class="img-fluid z-depth-0" width="100%" height="auto" title="sresp" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Behavioral response of a single larva across repeated presentations of the dark flash stimulus. The animal responds to the stimulus reliably when it is awake, alert, and swimming (speed != 0). Its response to the stimulus is abolished when it enters a quiescent, sleep-like state and stops swimming (speed = 0). </div> <p>Motivated by these observations, we utilized our tracking microscope to image brain activity while presenting dark flash stimuli to freely behaving fish as they spontaneously transitioned between wake and sleep-like states. Each experiment lasted 1.5 hours and dark flash stimuli were presented once per minute to prevent significant adaptation to the stimulus. Animals that did not exhibit any sleep-like quiescence were excluded from our analysis.</p> <h2 id="the-data"> <a name="data"></a>The data</h2> <p>As described above, experiments lasted 90 minutes and a stimulus was presented once per minute, resulting in 90 “trials” per dataset. Thus, we structured the data as shown below for further analysis.</p> <h4 id="behavioral-data">Behavioral data</h4> <div class="row justify-content-sm-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/roliFlexible/quiesc_vector-480.webp 480w,/assets/img/roliFlexible/quiesc_vector-800.webp 800w,/assets/img/roliFlexible/quiesc_vector-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/roliFlexible/quiesc_vector.png" class="img-fluid z-depth-0" width="100%" height="auto" title="qvec" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Internal state vector: Classify each trial, k, as wake vs. quiescence based on the speed measured in the 30 seconds prior to stimulus onset on each trial. </div> <h4 id="neural-data">Neural data</h4> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/roliFlexible/tensor-480.webp 480w,/assets/img/roliFlexible/tensor-800.webp 800w,/assets/img/roliFlexible/tensor-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/roliFlexible/tensor.png" class="img-fluid z-depth-0" width="100%" height="auto" title="qvec" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Neural data tensor (n x t x k): Neural activity recorded for each neuron, time point, and trial. Length of the time dimension is chosen according to the stimulus duration. </div> \[k = 90\] \[t = 36\] \[n = 57,651 \pm 7,839\] <h2 id="results"> <a name="results"></a>Results</h2> <h4 id="identify-relevant-neural-populations">Identify relevant neural populations</h4> <p>To study how the visuomotor computation driving the behavioral response to dark flashes changes as a function of internal state, it is necessary to first identify the neurons that compose this circuit. There are (at least) two critical components to this computation: Encode the visual stimulus and generate the motor output (high amplitude turn). To identify the neurons responsible for each, I used <a href="https://en.wikipedia.org/wiki/Reverse_correlation_technique" rel="external nofollow noopener" target="_blank">reverse correlation</a>. To briefly elaborate on this technique, I will use the dark flash stimulus.</p> <p>To identify visually responsive neurons, we “unwrap” our neural data tensor into a \(n\) x \(tk\) matrix and iterate over all \(n\) neurons to determine which of them contain information about the dark flash stimulus. The stimulus in the model is defined as a periodic impulse function – a vector of length \(tk\) which is zero everywhere except for at the stimulus onsets. The goal of model fitting is then to find each neuron’s “temporal impulse response filter.” Formally, we do this by fitting the Finite Impulse Response coefficients \(h\) in the model below, where \(\hat{r}_{i}(t)\) is the predicted activity of neuron \(i\) over all time. \(h\) are optimized to minimize the mean-squared-error between \(\hat{r}_{i}(t)\) and \(r_{i}(t)\), the true activity of the neuron. \(U\) are the number of time lags (FIR coefficients) to be fit. This approach allows us to remain agnostic to the particular temporal response profile that a neuron might have.</p> \[\hat{r}_{i}(t) = \sum_{u=0}^{U} h(t)s(t-u)\] <p>To identify neurons that encode high amplitude turns, we followed a very similar approach. The only difference being that the stimulus, \(s(t)\), was turn onsets, not dark flash onsets. We compared cross-validated model performance (\(R^2\)) to that of a null model in which “stimulus” onsets were scrambled in time. Neuron’s with significant \(R^2\) values for either the visual or turn model were deemed as belonging to the visuomotor circuit. An example of these populations for one fish is shown below, along with the response profiles of 3 example visual neurons and 3 example turn neurons.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/roliFlexible/responsive_cells-480.webp 480w,/assets/img/roliFlexible/responsive_cells-800.webp 800w,/assets/img/roliFlexible/responsive_cells-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/roliFlexible/responsive_cells.png" class="img-fluid z-depth-0" width="100%" height="auto" title="rcells" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Identification of dark flash and turn responsive neurons in one example fish. These cells comprise the putative visuomotor circuit that guides the dark flash evoked turning behavior. </div> <h4 id="leading-hypotheses">Leading hypotheses</h4> <p>Two exisitng hypotheses for “state-dependent” gating of behavior are:</p> <ol> <li>The gain hypothesis</li> <li>The null-space hypothesis</li> </ol> <p>The <strong>gain hypothesis</strong> posits that downstream motor neurons simply sum the activity of upstream visual neurons. If their summed response crosses some threshold, then the motor neurons are activated and a behavioral response is generated. Under this framework, decreasing the response magnitude of visual neurons to the visual stimulus during sleep could explain the state-dependent behavior we have observed.</p> <p>Alternatively, the <strong>null-space hypothesis</strong> proposes that there exist a very specific set of weights relating visual neuron activity to motor neuron activity and that these weights define a low-dimensional <em>communication subspace</em> linking visual neuron activity to motor neuron activity. Under this hypothesis, the idea is that sleep gates behavior either by causing a <em>rotation</em> of the visually evoked response or by causing a change in the communication weights such that visually evoked response the lies in the <em>null-space</em> of the motor neuron readout. This idea is illustrated below.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/roliFlexible/null_hypothesis-480.webp 480w,/assets/img/roliFlexible/null_hypothesis-800.webp 800w,/assets/img/roliFlexible/null_hypothesis-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/roliFlexible/null_hypothesis.png" class="img-fluid z-depth-0" width="100%" height="auto" title="rcells" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Cartoon sketch of the null-space hypothesis. On the bottom, the visually evoked response in neural state-space (where each dimension can be thought of as the activity of one neuron) is roughly orthogonal to the communication subspace. As a result, dark flashes do not drive activity in downstream motor neurons. </div> <h4 id="gain-hypothesis-is-not-consistent-with-behavior">Gain hypothesis is not consistent with behavior</h4> <p>To test the gain hypothesis, we split the data up into wake and quiescent trials on the basis of the animal’s speed, as described above. We then computed the mean dark flash response of each visual neuron under each condition. Below are two example neurons: One that is positively modulated by the wake state (left) and one that is negatively modulated by the wake state (right).</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/roliFlexible/example_cells-480.webp 480w,/assets/img/roliFlexible/example_cells-800.webp 800w,/assets/img/roliFlexible/example_cells-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/roliFlexible/example_cells.png" class="img-fluid z-depth-0" width="100%" height="auto" title="ex_cells" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Two example visual neurons. Top: raster plots showing neural activity over time during each trial, sorted by the fish's speed on each trial. Bottom: Peri-stimulus time histograms (PSTHs) summarizing the mean response to the stimulus under each condition (wake vs. quiescent). </div> <p>Across the population of visual neurons in all fish, we saw no clear trend for neurons to be postively or negatively modulated. Thus, we concluded that the gain hypothesis is unlikely to explain the state-dependent gating of fish behavior.</p> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/roliFlexible/df_summary-480.webp 480w,/assets/img/roliFlexible/df_summary-800.webp 800w,/assets/img/roliFlexible/df_summary-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/roliFlexible/df_summary.png" class="img-fluid z-depth-0" width="100%" height="auto" title="ex_cells" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Each point represents the dark flash response of one neuron under wake vs. quiescence conditions. All visual neurons across all fish are shown here. The response of a neuron was defined by taking the mean activity measured in the gray dashed box shown on the PSTHs of the example neurons above. </div> <h4 id="null-space-hypothesis-is-consistent-with-behavior">Null-space hypothesis is consistent with behavior</h4> <p>For testing the null-space hypothesis, we used Reduced Rank Regression (RRR) to identify the <a href="https://pubmed.ncbi.nlm.nih.gov/30770252/" rel="external nofollow noopener" target="_blank">communication subspace</a> linking visual neurons to motor neurons for each fish during each state (wake vs. quiescence). RRR identifies the dimensions of the source activity (the \(N_{visual}\) x \(k\) matrix of visual neurons) that are most predictive of the target activity (the \(N_{turn}\) x \(k\) matrix of motor neurons), subject to the constraint that the number of dimensions, \(m &lt;&lt; min(N_{visual}, N_{turn})\). This is sketched out graphically below.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/roliFlexible/rrr-480.webp 480w,/assets/img/roliFlexible/rrr-800.webp 800w,/assets/img/roliFlexible/rrr-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/roliFlexible/rrr.png" class="img-fluid z-depth-0" width="100%" height="auto" title="rrr" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Schematic of Reduced Rank Regression model. </div> <p>After fitting the RRR model, we selected the first dimension (\(m = 1\)) as our communication subspace and projected the dark flash evoked response of all visual neurons onto this subspace. Strikingly, we observed that the projected dark flash evoked activity during the quiescent state was almost zero, while during wake it was not. Thus, visually evoked response during sleep appears to lie in the null-space of the communication subspace. Said another way, the population of visual neurons seems to communicate information about the dark flash stimulus to motor neurons only during the wake state, consistent with the observed behavior.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/roliFlexible/projection-480.webp 480w,/assets/img/roliFlexible/projection-800.webp 800w,/assets/img/roliFlexible/projection-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/roliFlexible/projection.png" class="img-fluid z-depth-0" width="100%" height="auto" title="rrr" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Projection of dark flash evoked activity onto the communication subspace (left) or a random subspace (right). In both the wake and quiescent state, dark flash stimuli evoke large visual responses along a random subspace. However, on the communication subspace information about the stimulus is only present on wake trials. </div> <h2 id="conclusions-and-future-directions"> <a name="future"></a>Conclusions and future directions</h2> <p>The results shown here are preliminary, however, from our analysis and data thus far it seems that state-dependent sensorimotor gating could be achieved by dynamic modulation of the connectivity weights between visual and turn neuron populations which, in turn, causes the dark flash evoked response in visual neurons to have no impact on motor neurons during sleep.</p> <p>We are currently conducting further experiments to determine if this finding holds up across many fish. In addition, we are working on adapting the current computational method (RRR) to explicitly model the dynamically changing connectivity weights. This would allow us to fit one model to the data of each fish, rather than fitting a separate model for wake vs. quiescence. By doing this, we hope to 1. achieve more robust model fitting, as we could then leverage the full dataset to optimize model parameters and 2. discover the underlying gating dynamics in an unsupervised way, rather than imposing the somewhat arbitray division of wake vs. quiescence onto the data. This would be a novel approach to latent variable discovery from functional neural data, currently a very popular topic of research in computational and systems neuroscience.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Charles Heller. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>